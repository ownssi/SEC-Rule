{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eeef9aa",
   "metadata": {},
   "source": [
    "### Specifications\n",
    "<b>1. Data </b>\n",
    "- Fuzzy match results w/ columns: filename (which contains proposal release no and comment no), firm, firm ticker, match score\n",
    "- Formatted final rule data w/ columns: File No,Proposed Rule ID,Proposed Rule Date,Proposed Rule Text,Final Rule ID,Final Rule Date,Final Rule Text\n",
    "- Merging and cleaning the two files, I get rules_metadata.csv w/ columns: Release no.,Firms_Name,Firms_Ticker,Proposed date,Final date,Number of comments\n",
    "\n",
    "<b>2. Regression</b>\n",
    "- Data used: 1) time range: for each rule, use the final rule date to backtrack and forward 5 days. The final rule date and the next 5 days are considered \"Post,\" 2) CRSP daily data for the commenters of the rules and 12 sector ETFs during the period.\n",
    "- Regression formula: $Vol = \\alpha_i, \\beta_1 * post + \\beta_2 * post * commented + \\beta_3 * log(num comment) + \\epsilon$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76e8bf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# SEC Policy Impact — Simple Vol Regression (Permno-only)\n",
    "# Final sample: all commenters (from rules_metadata2.csv) + 12 sector ETFs\n",
    "# Event date: Final date\n",
    "# Window for regression: tau ∈ {-5,...,+5}\n",
    "# Vol measure: choose 21-day realized or Parkinson\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wrds\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# -----------------------\n",
    "# 0) CONFIG / PARAMETERS\n",
    "# -----------------------\n",
    "BASE = \"/Users/dorajyl/Desktop/w/UROP/Part 2. Data Cleaning/based_on_comment/data\"\n",
    "RULES_META_CSV = os.path.join(BASE, \"rules_metadata2.csv\")\n",
    "SECTOR_PERMNO_CSV = os.path.join(BASE, \"sp500_sector_permno.csv\")\n",
    "\n",
    "OUT = os.path.join(BASE, \"checkpoints_influential_simple\")\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# Influential filter (choose ONE mode) \n",
    "# for sample selection if run time is an issue\n",
    "FILTER_MODE = \"topN\"     # \"topN\" | \"percentile\" | \"mincount\"\n",
    "TOP_N = 75               # used if FILTER_MODE=\"topN\"\n",
    "PCTL = 0.90              # used if FILTER_MODE=\"percentile\"\n",
    "MIN_COMMENTS = 50        # used if FILTER_MODE=\"mincount\"\n",
    "\n",
    "# CRSP pull window (wide) & RV buffer\n",
    "EVENT_WINDOW = 90        # days pre/post for CRSP pull\n",
    "RV_BUFFER = 30           # extra days for realized-vol lookback\n",
    "\n",
    "# Winsorization switch\n",
    "APPLY_WINSOR = False\n",
    "WINSOR_P = 0.01          # 1% tails if APPLY_WINSOR=True\n",
    "\n",
    "CHUNK = 800  # permno chunk size for CRSP VALUES join\n",
    "\n",
    "# -----------------------\n",
    "# Helpers \n",
    "# -----------------------\n",
    "def choose_event_date(row):\n",
    "    \"\"\"Use Final date only (drop if missing).\"\"\"\n",
    "    if pd.notna(row['Final date']) and str(row['Final date']).strip():\n",
    "        return pd.to_datetime(row['Final date'])\n",
    "    return pd.NaT\n",
    "\n",
    "def split_permnos(s):\n",
    "    \"\"\"Split Firms_permno '19583; 53081; ...' into list of ints.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    pieces = [p.strip() for p in str(s).split(';')]\n",
    "    out = []\n",
    "    for p in pieces:\n",
    "        if not p:\n",
    "            continue\n",
    "        try:\n",
    "            out.append(int(p))\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "def parkinson_vol(H, L):\n",
    "    with np.errstate(all='ignore'):\n",
    "        rng = np.log(H / L)\n",
    "        var = (1.0 / (4.0 * np.log(2.0))) * (rng ** 2)\n",
    "        return np.sqrt(var)\n",
    "\n",
    "def realized_vol_21(ret):\n",
    "    r = pd.to_numeric(ret, errors='coerce')\n",
    "    return r.rolling(21, min_periods=15).std()\n",
    "\n",
    "def corwin_schultz_spread_series(g):\n",
    "    Ht, Lt = g['hi_q'], g['lo_q']\n",
    "    Hm1, Lm1 = Ht.shift(1), Lt.shift(1)\n",
    "    a_t  = (np.log(Ht / Lt))**2\n",
    "    a_m1 = (np.log(Hm1 / Lm1))**2\n",
    "    two  = (np.log(np.maximum(Ht, Hm1) / np.minimum(Lt, Lm1)))**2\n",
    "    gamma = np.sqrt(np.maximum(0.0, a_t + a_m1 - two))\n",
    "    S = 2.0 * (np.exp(gamma) - 1.0)\n",
    "    return S.clip(0, 1)\n",
    "\n",
    "def winsorize_by_year(df, col, p=0.01):\n",
    "    def _w(s):\n",
    "        lo, hi = s.quantile(p), s.quantile(1-p)\n",
    "        return s.clip(lo, hi)\n",
    "    return df.groupby(df['date'].dt.year)[col].transform(_w)\n",
    "\n",
    "def fetch_crsp_slice_for_permnos(db, permnos, start_date, end_date, chunk=800):\n",
    "    \"\"\"Server-side filter via a chunked VALUES join for performance.\"\"\"\n",
    "    out = []\n",
    "    cols = ['permno','date','prc','ret','vol','shrout','askhi','bidlo']\n",
    "    if len(permnos) == 0:\n",
    "        return pd.DataFrame(columns=cols)\n",
    "    for i in range(0, len(permnos), chunk):\n",
    "        chunk_ids = permnos[i:i+chunk]\n",
    "        values_rows = \",\".join(f\"({int(p)})\" for p in chunk_ids)\n",
    "        q = f\"\"\"\n",
    "            WITH perm_tmp(permno) AS (VALUES {values_rows})\n",
    "            SELECT d.permno, d.date, d.prc, d.ret, d.vol, d.shrout, d.askhi, d.bidlo\n",
    "            FROM crsp.dsf d\n",
    "            JOIN perm_tmp t ON d.permno = t.permno\n",
    "            WHERE d.date BETWEEN '{start_date}' AND '{end_date}'\n",
    "        \"\"\"\n",
    "        out.append(db.raw_sql(q))\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame(columns=cols)\n",
    "\n",
    "# -----------------------\n",
    "# Step 0: Load metadata\n",
    "# -----------------------\n",
    "req = [\n",
    "    'Release no.','Firms_Name','Firms_Ticker','Firms_permno',\n",
    "    'Proposed date','Final date','Number of comments'\n",
    "]\n",
    "meta = pd.read_csv(RULES_META_CSV)\n",
    "missing = [c for c in req if c not in meta.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in metadata: {missing}\")\n",
    "\n",
    "meta['event_date']   = meta.apply(choose_event_date, axis=1)\n",
    "meta['n_comments']   = pd.to_numeric(meta['Number of comments'], errors='coerce').fillna(0.0)\n",
    "meta['log_comments'] = np.log1p(meta['n_comments'])\n",
    "\n",
    "meta = meta.dropna(subset=['Release no.','event_date']).copy()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 0a: Filter \"influential\"\n",
    "# -------------------------------\n",
    "grp_max = meta.groupby('Release no.')['n_comments'].max()\n",
    "if FILTER_MODE == \"topN\":\n",
    "    keep_ids = grp_max.sort_values(ascending=False).head(TOP_N).index\n",
    "elif FILTER_MODE == \"percentile\":\n",
    "    cutoff = grp_max.quantile(PCTL)\n",
    "    keep_ids = grp_max[grp_max >= cutoff].index\n",
    "elif FILTER_MODE == \"mincount\":\n",
    "    keep_ids = grp_max[grp_max >= MIN_COMMENTS].index\n",
    "else:\n",
    "    raise ValueError(\"FILTER_MODE must be 'topN' or 'percentile' or 'mincount'\")\n",
    "\n",
    "meta_inf = meta[meta['Release no.'].isin(keep_ids)].copy()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 1: Commenter universe (permno from Firms_permno)\n",
    "# ------------------------------------------------------\n",
    "comment_rows = []\n",
    "for _, r in meta_inf.iterrows():\n",
    "    rel = r['Release no.']\n",
    "    ed  = r['event_date']\n",
    "    logc = r['log_comments']\n",
    "    permnos = split_permnos(r['Firms_permno'])\n",
    "    for p in permnos:\n",
    "        comment_rows.append({\n",
    "            'Release no.': rel,\n",
    "            'event_date':  ed,\n",
    "            'log_comments': logc,\n",
    "            'permno': p\n",
    "        })\n",
    "\n",
    "commenters_permno = (\n",
    "    pd.DataFrame(comment_rows)\n",
    "    .dropna(subset=['permno'])\n",
    "    .drop_duplicates()\n",
    ")\n",
    "commenters_permno['commented'] = 1\n",
    "\n",
    "if commenters_permno.empty:\n",
    "    raise RuntimeError(\"No commenter PERMNOs after filtering influential rules.\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 2: Sector ETF universe from sector_permno.csv\n",
    "# ------------------------------------------------------\n",
    "sector = pd.read_csv(SECTOR_PERMNO_CSV)\n",
    "if 'permno' not in sector.columns:\n",
    "    raise ValueError(\"sector_permno.csv must have a 'permno' column.\")\n",
    "sector['permno'] = pd.to_numeric(sector['permno'], errors='coerce')\n",
    "sector = sector.dropna(subset=['permno']).drop_duplicates()\n",
    "\n",
    "etf_rows = []\n",
    "for _, r in meta_inf[['Release no.','event_date','log_comments']].drop_duplicates().iterrows():\n",
    "    rel, ed, logc = r['Release no.'], r['event_date'], r['log_comments']\n",
    "    for p in sector['permno']:\n",
    "        etf_rows.append({\n",
    "            'Release no.': rel,\n",
    "            'event_date':  ed,\n",
    "            'log_comments': logc,\n",
    "            'permno': int(p)\n",
    "        })\n",
    "\n",
    "etf_permno = pd.DataFrame(etf_rows).drop_duplicates()\n",
    "etf_permno['commented'] = 0\n",
    "\n",
    "if etf_permno.empty:\n",
    "    raise RuntimeError(\"No sector ETF permnos found; check sector_permno.csv.\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 3: Exposure = commenters + sector ETFs\n",
    "# --------------------------------------------\n",
    "exposure = (\n",
    "    pd.concat([commenters_permno, etf_permno], ignore_index=True)\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "if exposure.empty:\n",
    "    raise RuntimeError(\"Exposure empty; check commenter permno and ETF mapping.\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Step 4: Pull CRSP dsf for permno universe\n",
    "# -------------------------------------------------------------------\n",
    "db = wrds.Connection()  # needs WRDS/Duo\n",
    "\n",
    "permno_universe = sorted(exposure['permno'].unique())\n",
    "event_min = (meta_inf['event_date'].min() - pd.Timedelta(days=EVENT_WINDOW + RV_BUFFER)).date()\n",
    "event_max = (meta_inf['event_date'].max() + pd.Timedelta(days=EVENT_WINDOW + RV_BUFFER)).date()\n",
    "\n",
    "crsp = fetch_crsp_slice_for_permnos(db, permno_universe, event_min, event_max, chunk=CHUNK)\n",
    "\n",
    "crsp['date'] = pd.to_datetime(crsp['date'])\n",
    "for c in ['prc','ret','vol','shrout','askhi','bidlo']:\n",
    "    crsp[c] = pd.to_numeric(crsp[c], errors='coerce')\n",
    "crsp['prc_abs'] = crsp['prc'].abs()\n",
    "crsp = crsp.sort_values(['permno','date']).reset_index(drop=True)\n",
    "crsp = crsp.rename(columns={'askhi':'hi_q','bidlo':'lo_q'})\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 5: Volatility proxies + winsorization (optional)\n",
    "# ------------------------------------------------------\n",
    "crsp['pk_vol'] = parkinson_vol(crsp['hi_q'], crsp['lo_q'])\n",
    "\n",
    "# use CRSP 'ret' when available; fallback to pct_change\n",
    "ret_used = crsp['ret'].copy()\n",
    "ret_fallback = crsp.groupby('permno')['prc_abs'].pct_change()\n",
    "ret_used = ret_used.where(ret_used.notna(), ret_fallback)\n",
    "\n",
    "crsp['rv21'] = (\n",
    "    ret_used\n",
    "    .groupby(crsp['permno'])\n",
    "    .apply(realized_vol_21)\n",
    ").reset_index(level=0, drop=True)\n",
    "\n",
    "cs_list = []\n",
    "for _, g in crsp.groupby('permno', sort=False):\n",
    "    cs_list.append(corwin_schultz_spread_series(g))\n",
    "crsp['cs_spread'] = pd.concat(cs_list).sort_index()\n",
    "crsp['cs_spread_smooth'] = crsp.groupby('permno')['cs_spread'] \\\n",
    "    .transform(lambda s: s.rolling(5, min_periods=3).median())\n",
    "\n",
    "for col in ['pk_vol','rv21','cs_spread_smooth']:\n",
    "    if APPLY_WINSOR:\n",
    "        crsp[col + '_w'] = winsorize_by_year(crsp, col, p=WINSOR_P)\n",
    "    else:\n",
    "        crsp[col + '_w'] = crsp[col]\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Step 6: Stack event windows \n",
    "# ----------------------------------------------\n",
    "panel = exposure.merge(crsp, on='permno', how='left')\n",
    "panel['tau']  = (panel['date'] - panel['event_date']).dt.days\n",
    "panel = panel[(panel['tau'] >= -EVENT_WINDOW) & (panel['tau'] <= EVENT_WINDOW)].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21014737",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1645266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Release no.', 'event_date', 'log_comments', 'permno', 'commented',\n",
       "       'date', 'prc', 'ret', 'vol', 'shrout', 'hi_q', 'lo_q', 'prc_abs',\n",
       "       'pk_vol', 'rv21', 'cs_spread', 'cs_spread_smooth', 'pk_vol_w', 'rv21_w',\n",
       "       'cs_spread_smooth_w', 'tau'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panel.to_csv(os.path.join(OUT, \"panel_full.csv\"), index=False)\n",
    "panel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9787409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Simple Vol Regression (rule FE; clustered by rule) ===\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    Vol   R-squared:                       0.470\n",
      "Model:                            OLS   Adj. R-squared:                  0.468\n",
      "Method:                 Least Squares   F-statistic:                 2.297e-08\n",
      "Date:                Tue, 18 Nov 2025   Prob (F-statistic):               1.00\n",
      "Time:                        09:23:13   Log-Likelihood:                 57428.\n",
      "No. Observations:               16492   AIC:                        -1.148e+05\n",
      "Df Residuals:                   16439   BIC:                        -1.143e+05\n",
      "Df Model:                          52                                         \n",
      "Covariance Type:              cluster                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "Intercept              -2.871e+08   1.96e+13  -1.46e-05      1.000   -3.85e+13    3.85e+13\n",
      "C(rule_id)[T.34-69077]  2.973e+07   1.45e+12   2.05e-05      1.000   -2.84e+12    2.84e+12\n",
      "C(rule_id)[T.34-69490]  3.057e+07   1.73e+12   1.77e-05      1.000   -3.39e+12    3.39e+12\n",
      "C(rule_id)[T.34-69491]  2.454e+08   1.13e+13   2.18e-05      1.000   -2.21e+13    2.21e+13\n",
      "C(rule_id)[T.34-69606]  2.973e+07   1.23e+12   2.41e-05      1.000   -2.42e+12    2.42e+12\n",
      "C(rule_id)[T.34-70277] -9.118e+07   5.41e+12  -1.69e-05      1.000   -1.06e+13    1.06e+13\n",
      "C(rule_id)[T.34-71699]   2.21e+08   1.37e+13   1.61e-05      1.000   -2.69e+13    2.69e+13\n",
      "C(rule_id)[T.34-71958]  6.979e+07   3.34e+12   2.09e-05      1.000   -6.55e+12    6.55e+12\n",
      "C(rule_id)[T.34-72440]   2.21e+08   1.46e+13   1.52e-05      1.000   -2.86e+13    2.86e+13\n",
      "C(rule_id)[T.34-74245]  9.102e+07   4.79e+12    1.9e-05      1.000   -9.38e+12    9.38e+12\n",
      "C(rule_id)[T.34-74834]  8.875e+07   5.76e+12   1.54e-05      1.000   -1.13e+13    1.13e+13\n",
      "C(rule_id)[T.34-74835] -8.543e+06   5.46e+11  -1.57e-05      1.000   -1.07e+12    1.07e+12\n",
      "C(rule_id)[T.34-75612]  1.327e+08   5.34e+12   2.49e-05      1.000   -1.05e+13    1.05e+13\n",
      "C(rule_id)[T.34-75845]   2.21e+08   1.18e+13   1.88e-05      1.000   -2.31e+13    2.31e+13\n",
      "C(rule_id)[T.34-75976]  1.428e+08   9.48e+12   1.51e-05      1.000   -1.86e+13    1.86e+13\n",
      "C(rule_id)[T.34-75977]  1.619e+08   1.15e+13   1.41e-05      1.000   -2.25e+13    2.25e+13\n",
      "C(rule_id)[T.34-76474]  5.289e+07   2.41e+12    2.2e-05      1.000   -4.72e+12    4.72e+12\n",
      "C(rule_id)[T.34-76620]  4.593e+07   2.33e+12   1.98e-05      1.000   -4.56e+12    4.56e+12\n",
      "C(rule_id)[T.34-76922]   2.21e+08   1.17e+13   1.89e-05      1.000   -2.29e+13    2.29e+13\n",
      "C(rule_id)[T.34-76958]  4.593e+07   3.76e+12   1.22e-05      1.000   -7.37e+12    7.37e+12\n",
      "C(rule_id)[T.34-77157]  1.793e+08    1.4e+13   1.28e-05      1.000   -2.74e+13    2.74e+13\n",
      "C(rule_id)[T.34-78309]  2.117e+07   5.33e+11   3.97e-05      1.000   -1.05e+12    1.05e+12\n",
      "C(rule_id)[T.34-78962]  8.445e+07   5.22e+12   1.62e-05      1.000   -1.02e+13    1.02e+13\n",
      "C(rule_id)[T.34-78963]  1.902e+08   4.57e+12   4.17e-05      1.000   -8.95e+12    8.95e+12\n",
      "C(rule_id)[T.34-80130]   2.89e+07   8.52e+11   3.39e-05      1.000   -1.67e+12    1.67e+12\n",
      "C(rule_id)[T.34-82373]  2.037e+08   2.06e+12    9.9e-05      1.000   -4.03e+12    4.03e+12\n",
      "C(rule_id)[T.34-82873] -2.162e+07   3.95e+11  -5.47e-05      1.000   -7.74e+11    7.74e+11\n",
      "C(rule_id)[T.34-83062] -1.301e+08   2.22e+12  -5.86e-05      1.000   -4.35e+12    4.35e+12\n",
      "C(rule_id)[T.34-83063] -7.582e+07   4.55e+12  -1.67e-05      1.000   -8.92e+12    8.92e+12\n",
      "C(rule_id)[T.34-83557] -7.201e+06   3.91e+11  -1.84e-05      1.000   -7.66e+11    7.66e+11\n",
      "C(rule_id)[T.34-84225]  2.454e+08   1.33e+13   1.85e-05      1.000   -2.61e+13    2.61e+13\n",
      "C(rule_id)[T.34-84289]   2.21e+08   1.53e+13   1.44e-05      1.000   -3.01e+13    3.01e+13\n",
      "C(rule_id)[T.34-84409] -1.893e-06      0.103  -1.84e-05      1.000      -0.202       0.202\n",
      "C(rule_id)[T.34-84861]  2.037e+08   1.23e+13   1.65e-05      1.000   -2.41e+13    2.41e+13\n",
      "C(rule_id)[T.34-85814]  3.229e+07    1.9e+12    1.7e-05      1.000   -3.72e+12    3.72e+12\n",
      "C(rule_id)[T.34-85823]  1.099e+08   7.45e+12   1.48e-05      1.000   -1.46e+13    1.46e+13\n",
      "C(rule_id)[T.34-86304]  1.327e+08   8.08e+12   1.64e-05      1.000   -1.58e+13    1.58e+13\n",
      "C(rule_id)[T.34-87115] -1.484e+07   1.06e+12   -1.4e-05      1.000   -2.08e+12    2.08e+12\n",
      "C(rule_id)[T.34-87193]  1.202e+08   7.81e+12   1.54e-05      1.000   -1.53e+13    1.53e+13\n",
      "C(rule_id)[T.34-87457] -1.004e+08   7.11e+12  -1.41e-05      1.000   -1.39e+13    1.39e+13\n",
      "C(rule_id)[T.34-87458] -1.248e+08   7.61e+12  -1.64e-05      1.000   -1.49e+13    1.49e+13\n",
      "C(rule_id)[T.34-87607] -2.127e+08   1.34e+13  -1.58e-05      1.000   -2.63e+13    2.63e+13\n",
      "C(rule_id)[T.34-87783]  8.181e+06   4.94e+11   1.66e-05      1.000   -9.68e+11    9.68e+11\n",
      "C(rule_id)[T.IA-3893]   1.375e+08    9.1e+12   1.51e-05      1.000   -1.78e+13    1.78e+13\n",
      "C(rule_id)[T.IA-4091]   4.931e+07   2.14e+12    2.3e-05      1.000    -4.2e+12     4.2e+12\n",
      "C(rule_id)[T.IA-4697]    2.21e+08    1.3e+13    1.7e-05      1.000   -2.55e+13    2.55e+13\n",
      "C(rule_id)[T.IA-5407]   1.834e+07   9.39e+11   1.95e-05      1.000   -1.84e+12    1.84e+12\n",
      "C(rule_id)[T.IC-31184]  5.801e+07   3.57e+12   1.63e-05      1.000   -6.99e+12    6.99e+12\n",
      "C(rule_id)[T.IC-31933] -2.127e+08   1.67e+13  -1.27e-05      1.000   -3.28e+13    3.28e+13\n",
      "C(rule_id)[T.IC-33046]  6.216e+07   3.63e+12   1.71e-05      1.000   -7.12e+12    7.12e+12\n",
      "C(rule_id)[T.IC-33658]  1.327e+08   6.05e+12   2.19e-05      1.000   -1.19e+13    1.19e+13\n",
      "post                      -0.0037      0.000     -8.232      0.000      -0.005      -0.003\n",
      "post:commented             0.0061      0.007      0.918      0.359      -0.007       0.019\n",
      "log_comments            6.018e+07   2.84e+12   2.12e-05      1.000   -5.57e+12    5.57e+12\n",
      "==============================================================================\n",
      "Omnibus:                     9598.368   Durbin-Watson:                   0.327\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           169429.313\n",
      "Skew:                           2.449   Prob(JB):                         0.00\n",
      "Kurtosis:                      17.919   Cond. No.                     2.70e+14\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are robust to cluster correlation (cluster)\n",
      "[2] The smallest eigenvalue is 5.47e-24. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorajyl/Desktop/w/UROP/venv/lib/python3.9/site-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 53, but rank is 3\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Step 7: Simple regression on τ ∈ {-5,...,+5}\n",
    "#     Vol_{i,t} = α_i + β1*post + β2*post*commented + β3*log_comments + ε\n",
    "# ==========================================================\n",
    "# Tight analysis window for regression\n",
    "EVENT_WINDOW_A = 5       # tau ∈ [-5, +5]\n",
    "panel_a = panel[(panel['tau'] >= -EVENT_WINDOW_A) & (panel['tau'] <= EVENT_WINDOW_A)].copy()\n",
    "\n",
    "# post = 1 if tau >= 0 (final date and above)\n",
    "panel_a['post'] = (panel_a['tau'] >= 0).astype(int)\n",
    "# commented = 1 for commenters, 0 for sector ETFs\n",
    "panel_a['commented'] = panel_a['commented'].astype(int)\n",
    "# Vol outcome\n",
    "panel_a['Vol'] = panel_a['rv21']\n",
    "# log_comments (per rule) — fill if any missing\n",
    "panel_a['log_comments'] = panel_a['log_comments'].fillna(0)\n",
    "# rule fixed effects\n",
    "panel_a['rule_id'] = panel_a['Release no.'].astype(str)\n",
    "panel_a.dropna(subset=['Vol'], inplace=True)\n",
    "# Regression model\n",
    "mod = smf.ols(\n",
    "    'Vol ~ post + post:commented + log_comments + C(rule_id)',\n",
    "    data=panel_a\n",
    ")\n",
    "\n",
    "# cluster by rule\n",
    "res = mod.fit(\n",
    "    cov_type='cluster',\n",
    "    cov_kwds={'groups': panel_a['rule_id']}\n",
    ")\n",
    "with open(os.path.join(OUT, \"result_rv21.txt\"), \"w\") as f:\n",
    "    f.write(str(res.summary()))\n",
    "\n",
    "print(\"\\n=== Simple Vol Regression (rule FE; clustered by rule) ===\")\n",
    "print(res.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
