# -*- coding: utf-8 -*-
"""(Personal working)Pre 2013 with DF rigging

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-9flQtE5f-T-NVEPs_k0KDgbTkRutCdZ

# Library and necessary functions
"""

from bs4 import BeautifulSoup, NavigableString, Tag
import requests
import re
import csv
import pandas as pd
import sqlite3

def getdoc(url):
    # request for HTML document of given url
    response = requests.get(url)
      
    #JSON format
    return response.text

def wordTnum(string):

  if "Dec. " in string:
    string = string.replace("Dec.", "12/")
  elif "Nov. " in string:
    string = string.replace("Nov.", "11/")
  elif "Oct. " in string:
    string = string.replace("Oct.", "10/")
  elif "Sep. " in string:
    string = string.replace("Sep.", "09/")
  elif "Aug. " in string:
      string = string.replace("Aug.", "08/")
  elif "Jul. " in string:
      string = string.replace("Jul.", "07/")
  elif "Jun. " in string:
      string = string.replace("Jun.", "06/")
  elif "May " in string:
      string = string.replace("May", "05/")
  elif "Apr. " in string:
      string = string.replace("Apr.", "04/")
  elif "Mar. " in string:
      string = string.replace("Mar.", "03/")
  elif "Feb. " in string:
      string = string.replace("Feb.", "02/")
  elif "Jan. " in string:
      string = string.replace("Jan.", "01/")

  
  if " 1, " in string:
    string = string.replace(" 1, ", "01/")
  elif " 2, " in string:
    string = string.replace(" 2, ", "02/")
  elif " 3, " in string:
    string = string.replace(" 3, ", "03/")
  elif " 4, " in string:
    string = string.replace(" 4, ", "04/")
  elif " 5, " in string:
    string = string.replace(" 5, ", "05/")
  elif " 6, " in string:
    string = string.replace(" 6, ", "06/")
  elif " 7, " in string:
    string = string.replace(" 7, ", "07/")
  elif " 8, " in string:
    string = string.replace(" 8, ", "08/")
  elif " 9, " in string:
    string = string.replace(" 9, ", "09/")
  else:
    string = string.replace("/ ","/")
    string = string.replace(", ","/")
    

  return string

"""# Scrapper and Matcher

## Scrapping and matching in bulk

Batch scrapper (5 years)
"""

url_to_scrape1 = "https://www.sec.gov/rules/proposed/proposedarchive/proposed1995.shtml"
url_to_scrape2 = "https://www.sec.gov/rules/proposed/proposedarchive/proposed1996.shtml"
url_to_scrape3 = "https://www.sec.gov/rules/proposed/proposedarchive/proposed1997.shtml"
url_to_scrape4 = "https://www.sec.gov/rules/proposed/proposedarchive/proposed1998.shtml"
url_to_scrape5 = "https://www.sec.gov/rules/proposed/proposedarchive/proposed1999.shtml"

html_document1 = getdoc(url_to_scrape1)
html_document2 = getdoc(url_to_scrape2)
html_document3 = getdoc(url_to_scrape3)
html_document4 = getdoc(url_to_scrape4)
html_document5 = getdoc(url_to_scrape5)

soup1 = BeautifulSoup(html_document1)
table1 = soup1.find("table", {"border" : "0","cellpadding": "4", "cellspacing": "0"}).find('tr')
soup2 = BeautifulSoup(html_document2)
table2 = soup2.find("table", {"border" : "0","cellpadding": "4", "cellspacing": "0"}).find('tr')
soup3 = BeautifulSoup(html_document3)
table3 = soup3.find("table", {"border" : "0","cellpadding": "4", "cellspacing": "0"}).find('tr')
soup4 = BeautifulSoup(html_document4)
table4 = soup4.find("table", {"border" : "0","cellpadding": "4", "cellspacing": "0"}).find('tr')
soup5 = BeautifulSoup(html_document5)
table5 = soup5.find("table", {"border" : "0","cellpadding": "4", "cellspacing": "0"}).find('tr')

list_table = [table1, table2, table3, table4, table5]
#list_table = [table1, table2, table3] #Sometimes, you will need three years of batch

file = open('list.csv','w')
writer = csv.writer(file)
headers = (["File No","Proposed Rule ID", "Proposed Rule Date", "Proposed Rule Text",
            "Final Rule ID", "Final Rule Date", "Final Rule Text"])
writer.writerow(headers)
file.close()

counter = 0
for table in list_table:
  for s in table.findNextSiblings()[1:]:
    Qrow_condition = (s.find('b',text = re.compile('Fourth *Quarter')) or s.find('b',text = re.compile('Third *Quarter'))
    or s.find('b',text = re.compile('Second *Quarter')) or s.find('b',text = re.compile('First *Quarter')) )
    if Qrow_condition:
      continue #Quarter passing
      print("Quarter detected!") #This won't be printed on

    #print(s) #Debugging code
    if not s.find('a',href = True):
      continue
    print("Text: ", "https://www.sec.gov" + s.find('a',href = True)['href'])
    print("ID:",s.find('td').text.strip())
    print("Date: ", wordTnum(s.find('td').findNextSibling().text).strip())
    
    f_Num1 = s.find('b',text = re.compile('File No'))
    f_Num2 = s.find('i',text = re.compile('File No'))
    if f_Num1 and isinstance(f_Num1.nextSibling,NavigableString):
      print("File No: ", f_Num1.nextSibling.strip())
    elif f_Num2 and isinstance(f_Num2.nextSibling,NavigableString):
      print("File No: ", f_Num2.nextSibling.strip())
    else:
      counter += 1
      print("No File number")
      print("###############################################")
      continue
    print("==========================================")

    rule_id = s.find('td').text.strip()
    date = wordTnum(s.find('td').findNextSibling().text.strip())
    url = "https://www.sec.gov" + s.find('a',href = True)['href']
    if f_Num1 and isinstance(f_Num1.nextSibling,NavigableString):
      file_no = f_Num1.nextSibling.strip()
    elif f_Num2 and isinstance(f_Num2.nextSibling,NavigableString):
      file_no = f_Num2.nextSibling.strip()
    

    if ',' in file_no:
      
      for each_fnum in file_no.split(", "):
        file = open('list.csv','a')
        writer = csv.writer(file)
        headers = ([each_fnum,rule_id, date, url,'','',''])
        writer.writerow(headers)
        file.close

    else:
      file = open('list.csv','a')
      writer = csv.writer(file)
      headers = ([file_no,rule_id, date, url,'','',''])
      writer.writerow(headers)
      file.close()

print("Number of Proposed rule without File No: ",counter)

"""Batch Matcher (5 years)"""

df = pd.read_csv("list.csv")
df.fillna('', inplace=True)

url_to_scrape1 = "https://www.sec.gov/rules/final/finalarchive/finalarchive1995.shtml"
url_to_scrape2 = "https://www.sec.gov/rules/final/finalarchive/finalarchive1996.shtml"
url_to_scrape3 = "https://www.sec.gov/rules/final/finalarchive/finalarchive1997.shtml"
url_to_scrape4 = "https://www.sec.gov/rules/final/finalarchive/finalarchive1998.shtml"
url_to_scrape5 = "https://www.sec.gov/rules/final/finalarchive/finalarchive1999.shtml"
html_document1 = getdoc(url_to_scrape1)
html_document2 = getdoc(url_to_scrape2)
html_document3 = getdoc(url_to_scrape3)
html_document4 = getdoc(url_to_scrape4)
html_document5 = getdoc(url_to_scrape5)

soup1 = BeautifulSoup(html_document1)
table1 = soup1.find("table", {"border" : "0","cellpadding": "4", "cellspacing": "0"}).find('tr')
soup2 = BeautifulSoup(html_document2)
table2 = soup2.find("table", {"border" : "0","cellpadding": "4", "cellspacing": "0"}).find('tr')
soup3 = BeautifulSoup(html_document3)
table3 = soup3.find("table", {"border" : "0","cellpadding": "4", "cellspacing": "0"}).find('tr')
soup4 = BeautifulSoup(html_document4)
table4 = soup4.find("table", {"border" : "0","cellpadding": "4", "cellspacing": "0"}).find('tr')
soup5 = BeautifulSoup(html_document5)
table5 = soup5.find("table", {"border" : "0","cellpadding": "4", "cellspacing": "0"}).find('tr')

list_table = [table1, table2, table3, table4, table5]
#list_table = [table1, table2, table3] #Sometimes, you will need three years of batch

counter = 0
for table in list_table:
  for s in table.findNextSiblings()[1:]:
    Qrow_condition = (s.find('b',text = re.compile('Fourth *Quarter')) or s.find('b',text = re.compile('Third *Quarter'))
    or s.find('b',text = re.compile('Second *Quarter')) or s.find('b',text = re.compile('First *Quarter')) )
    if Qrow_condition:
      continue #Quarter passing
      print("Quarter detected!") #This won't be printed on

    #print(s) #Debugging code
    print("Text: ", "https://www.sec.gov" + s.find('a',href = True)['href'])
    print("ID:",s.find('td').text.strip())
    print("Date: ", wordTnum(s.find('td').findNextSibling().text))

    f_Num1 = s.find('b',text = re.compile('File No'))
    f_Num2 = s.find('i',text = re.compile('File No'))
    if f_Num1 and isinstance(f_Num1.nextSibling,NavigableString):
      print("File No: ", f_Num1.nextSibling.strip())
      f_Num = f_Num1
    elif f_Num2 and isinstance(f_Num2.nextSibling,NavigableString):
      print("File No: ", f_Num2.nextSibling.strip())
      f_Num = f_Num2
    else:
      counter += 1
      print("No File number")
      print("###############################################")
      continue
    print("==========================================")

    condition = df["File No"] == f_Num.nextSibling.strip()
    df.loc[condition,"Final Rule ID"] = df["Final Rule ID"].astype(str) + s.find('td').text.strip() + ', '
    df.loc[condition,"Final Rule Date"] = df["Final Rule Date"].astype(str) + wordTnum(s.find('td').findNextSibling().text) + ', '
    df.loc[condition,"Final Rule Text"] = df["Final Rule Text"].astype(str) + "https://www.sec.gov" + s.find('a',href = True)['href'] + ', '

print("Number of Final rule without File No: ",counter)
df.to_csv('list.csv',index=False)

"""## One year scrapper and matcher

Scrapper
"""

url_to_scrape = "https://www.sec.gov/rules/proposed/proposedarchive/proposed2001.shtml"
#1998 has few exceptions

html_document = getdoc(url_to_scrape)
soup = BeautifulSoup(html_document)
table = soup.find("table", {"border" : "0","cellpadding": "4", "cellspacing": "0"}).find('tr')


#print(table.find_all('tr')) #Debugging code
#print("=============================")
file = open('list.csv','w')
writer = csv.writer(file)
headers = (["File No","Proposed Rule ID", "Proposed Rule Date", "Proposed Rule Text",
            "Final Rule ID", "Final Rule Date", "Final Rule Text"])
writer.writerow(headers)
file.close()

counter = 0
for s in table.findNextSiblings()[1:]:
  if 'Quarter</b>' in str(s):
    continue #Quarter passing
    print("This shouldn't be printed")

  if not s.find('a',href = True):
      continue
  
  #print(s.find('td')) #Debugging code
  print("Text: ", "https://www.sec.gov" + s.find('a',href = True)['href'])
  print("ID:",s.find('td').text.strip())
  print("Date: ", wordTnum(s.find('td').findNextSibling().text))

  f_Num1 = s.find('b',text = re.compile('File No'))
  f_Num2 = s.find('i',text = re.compile('File No'))
  if f_Num1 and isinstance(f_Num1.nextSibling,NavigableString):
    print("File No: ", f_Num1.nextSibling.strip())
  elif f_Num2 and isinstance(f_Num2.nextSibling,NavigableString):
    print("File No: ", f_Num2.nextSibling.strip())
  else:
    counter += 1
    print("No File number")
    print("###################################")
    continue
  print("==========================================")

  rule_id = s.find('td').text
  date = wordTnum(s.find('td').findNextSibling().text)
  url = "https://www.sec.gov" + s.find('a',href = True)['href']
  file_no = f_Num1.nextSibling.strip() if f_Num1.nextSibling else f_Num2.nextSibling.strip()

  if ',' in file_no:
    
    for each_fnum in file_no.split(", "):
      file = open('list.csv','a')
      writer = csv.writer(file)
      headers = ([each_fnum,rule_id, date, url,"","",""])
      writer.writerow(headers)
      file.close

  else:
    file = open('list.csv','a')
    writer = csv.writer(file)
    headers = ([file_no,rule_id, date, url,"","",""])
    writer.writerow(headers)
    file.close()

"""Matcher"""

df = pd.read_csv("list.csv")
df.fillna('', inplace=True)

url_to_scrape = "https://www.sec.gov/rules/final/finalarchive/finalarchive1997.shtml"

html_document = getdoc(url_to_scrape)
soup = BeautifulSoup(html_document)

table = soup.find("table", {"border" : "0","cellpadding": "4", "cellspacing": "0"}).find('tr')

counter = 0
for s in table.findNextSiblings()[1:]:

  Qrow_condition = (s.find('b',text = re.compile('Fourth *Quarter')) or s.find('b',text = re.compile('Third *Quarter'))
  or s.find('b',text = re.compile('Second *Quarter')) or s.find('b',text = re.compile('First *Quarter')) )
  if Qrow_condition:
    continue #Quarter passing
    print("Quarter detected!")

  #print(s) #Debugging code
  print("Text: ", "https://www.sec.gov" + s.find('a',href = True)['href'])
  print("ID:",s.find('td').text)
  print("Date: ", wordTnum(s.find('td').findNextSibling().text))
  f_Num = s.find('b',text = re.compile('File No'))
  if f_Num and isinstance(f_Num.nextSibling, NavigableString):
    print("File No: ",f_Num.nextSibling.strip())
  else:
    print("No File number")
    print("#################################")
    counter += 1
    continue

  print("==========================================")

  condition = df["File No"] == f_Num.nextSibling.strip()
  df.loc[condition,"Final Rule ID"] = s.find('td').text
  df.loc[condition,"Final Rule Date"] = wordTnum(s.find('td').findNextSibling().text)
  df.loc[condition,"Final Rule Text"] = "https://www.sec.gov" + s.find('a',href = True)['href']


df.to_csv('list.csv',index=False)
print("Number of Final rule without File No: ",counter)

"""# Data frame rigging"""

con = sqlite3.connect('list.db')
df = pd.read_csv('list.csv')
cur = con.cursor()
df.to_sql("MyTable", con, if_exists='append', index=False)

res = cur.execute('''CREATE TABLE NewTable
 AS SELECT "Final Rule ID", "Final Rule Date", "Final Rule Text", 
 (LENGTH("Final Rule ID")-LENGTH(REPLACE("Final Rule ID",",",""))) FROM MyTable;''')

# load data
df = pd.read_csv('list.csv')

#Run if 1996 rule is contained
#mask = df["Final Rule ID"] == re.complie("33-7431, ")
#df["Final Rule ID"][24] = "33-7431, "
#df.loc[mask,"Final Rule ID"] = 

df['Final Rule ID']=df['Final Rule ID'].str[:-2]
df['Final Rule Date']=df['Final Rule Date'].str[:-2]
df['Final Rule Text']=df['Final Rule Text'].str[:-2]

df['Final Rule ID'] = df['Final Rule ID'].str.split(', ')
df['Final Rule Date'] = df['Final Rule Date'].str.split(', ')
df['Final Rule Text'] = df['Final Rule Text'].str.split(', ')

columns = ["Final Rule ID", "Final Rule Date", "Final Rule Text"]
df = df.explode(column=columns).reset_index(drop=False)

df.to_csv('formatted.csv', index = False)
display(df)