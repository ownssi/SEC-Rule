# -*- coding: utf-8 -*-
"""(Personal working)Post 2013 with DF rigging

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JL1aPspfu-G81WjmWabjKWW82OopZgxY

Welcome! To use scrapper, first run the "Library and necessay function section".

# Library and necessary function
"""

from bs4 import BeautifulSoup, NavigableString, Tag
import requests
import re
import csv
import pandas as pd

def getdoc(url):
    # request for HTML document of given url
    response = requests.get(url)
      
    #JSON format
    return response.text

def wordTnum(string):

  if "Dec. " in string:
    string = string.replace("Dec.", "12/")
  elif "Nov. " in string:
    string = string.replace("Nov.", "11/")
  elif "Oct. " in string:
    string = string.replace("Oct.", "10/")
  elif "Sep. " in string:
    string = string.replace("Sep.", "09/")
  elif "Aug. " in string:
      string = string.replace("Aug.", "08/")
  elif "Jul. " in string:
      string = string.replace("Jul.", "07/")
  elif "Jun. " in string:
      string = string.replace("Jun.", "06/")
  elif "May " in string:
      string = string.replace("May", "05/")
  elif "Apr. " in string:
      string = string.replace("Apr.", "04/")
  elif "Mar. " in string:
      string = string.replace("Mar.", "03/")
  elif "Feb. " in string:
      string = string.replace("Feb.", "02/")
  elif "Jan. " in string:
      string = string.replace("Jan.", "01/")

  
  if " 1, " in string:
    string = string.replace(" 1, ", "01/")
  elif " 2, " in string:
    string = string.replace(" 2, ", "02/")
  elif " 3, " in string:
    string = string.replace(" 3, ", "03/")
  elif " 4, " in string:
    string = string.replace(" 4, ", "04/")
  elif " 5, " in string:
    string = string.replace(" 5, ", "05/")
  elif " 6, " in string:
    string = string.replace(" 6, ", "06/")
  elif " 7, " in string:
    string = string.replace(" 7, ", "07/")
  elif " 8, " in string:
    string = string.replace(" 8, ", "08/")
  elif " 9, " in string:
    string = string.replace(" 9, ", "09/")
  else:
    string = string.replace("/ ","/")
    string = string.replace(", ","/")
    

  return string

"""# Scrapper and Matcher

## Scrapping in bulk

The varialbes url_to_scrape1-5 are urls of the years you want to scrap.
After, scrapper finished its run. You will find list.csv in the Files menu(It's located right side of the menu with folder shape)

If you want to scrap less than five years, then you can edit the elements of list_table, the elements in the list will be url(year of proposed rule) to be scrapped.

Bulk Scrapper
"""

url_to_scrape1 = "https://www.sec.gov/rules/proposed/proposedarchive/proposed2020.shtml"
url_to_scrape2 = "https://www.sec.gov/rules/proposed/proposedarchive/proposed2021.shtml"
url_to_scrape3 = "https://www.sec.gov/rules/proposed/proposedarchive/proposed2022.shtml"
url_to_scrape4 = "https://www.sec.gov/rules/proposed/proposedarchive/proposed2014.shtml"
url_to_scrape5 = "https://www.sec.gov/rules/proposed/proposedarchive/proposed2013.shtml"

html_document1 = getdoc(url_to_scrape1)
html_document2 = getdoc(url_to_scrape2)
html_document3 = getdoc(url_to_scrape3)
html_document4 = getdoc(url_to_scrape4)
html_document5 = getdoc(url_to_scrape5)

soup1 = BeautifulSoup(html_document1)
table1 = soup1.find("table").find('tr')
soup2 = BeautifulSoup(html_document2)
table2 = soup2.find("table").find('tr')
soup3 = BeautifulSoup(html_document3)
table3 = soup3.find("table").find('tr')
soup4 = BeautifulSoup(html_document4)
table4 = soup4.find("table").find('tr')
soup5 = BeautifulSoup(html_document5)
table5 = soup5.find("table").find('tr')

list_table = [table1, table2, table3, table4, table5]
list_table = [table1, table2, table3] #Sometimes, you will just need three years of batch

file = open('list.csv','w')
writer = csv.writer(file)
headers = (["File No","Proposed Rule ID", "Proposed Rule Date", "Proposed Rule Text",
            "Final Rule ID", "Final Rule Date", "Final Rule Text"])
writer.writerow(headers)
file.close()

counter = 0
for table in list_table:
  for s in table.findNextSiblings()[1:]:
    Qrow_condition = (s.find('b',text = re.compile('Fourth *Quarter')) or s.find('b',text = re.compile('Third *Quarter'))
    or s.find('b',text = re.compile('Second *Quarter')) or s.find('b',text = re.compile('First *Quarter')) )
    if Qrow_condition:
      continue #Quarter passing
      print("Quarter detected!") #This won't be printed on

    #print(s) #Debugging code
    if not s.find('a',href = True):
      continue
    print("Text: ", "https://www.sec.gov" + s.find('a',href = True)['href'])
    print("ID:",s.find('td').text.strip())
    print("Date: ", wordTnum(s.find('td').findNextSibling().text).strip())
    f_Num1 = s.find('b',text = re.compile('File No'))
    f_Num2 = s.find('i',text = re.compile('File No'))
    f_Num3 = s.find('strong',text = re.compile('File No'))

    if f_Num1 and isinstance(f_Num1.nextSibling,NavigableString):
      print("File No: ", f_Num1.nextSibling.strip())
    elif f_Num2 and isinstance(f_Num2.nextSibling,NavigableString):
      print("File No: ", f_Num2.nextSibling.strip())
    elif f_Num3 and isinstance(f_Num3.nextSibling,NavigableString):
      print("File No: ", f_Num3.nextSibling.strip())
    else:
      counter += 1
      print("No File number")
      print("###############################################")
      continue
    print("==========================================")


    rule_id = s.find('td').text.strip()
    date = wordTnum(s.find('td').findNextSibling().text.strip())
    url = "https://www.sec.gov" + s.find('a',href = True)['href']
    if f_Num1 and isinstance(f_Num1.nextSibling,NavigableString):
      file_no = f_Num1.nextSibling.strip()
    elif f_Num2 and isinstance(f_Num2.nextSibling,NavigableString):
      file_no = f_Num2.nextSibling.strip()
    elif f_Num3 and isinstance(f_Num3.nextSibling,NavigableString):
      file_no = f_Num3.nextSibling.strip()

    if ',' in file_no:
      
      for each_fnum in file_no.split(", "):
        file = open('list.csv','a')
        writer = csv.writer(file)
        headers = ([each_fnum,rule_id, date, url,'','',''])
        writer.writerow(headers)
        file.close

    else:
      file = open('list.csv','a')
      writer = csv.writer(file)
      headers = ([file_no,rule_id, date, url,'','',''])
      writer.writerow(headers)
      file.close()

print("Number of Proposed rule without File No: ",counter)

"""## Single year scrapper

This is single year scrapper, but the function is excatly the same. I just made bulk scrapper for convenience, and single scrapper for debugging.

Scrapper
"""

url_to_scrape = "https://www.sec.gov/rules/proposed/proposedarchive/proposed2013.shtml"

html_document = getdoc(url_to_scrape)
soup = BeautifulSoup(html_document)
table = soup.find("table").find('tr')

file = open('list.csv','w')
file = open('list.csv','w')
writer = csv.writer(file)
headers = (["File No","Proposed Rule ID", "Proposed Rule Date", "Proposed Rule Text",
            "Final Rule ID", "Final Rule Date", "Final Rule Text"])
writer.writerow(headers)
file.close()

counter = 0
for s in table.findNextSiblings()[1:]:
  Qrow_condition = (s.find('b',text = re.compile('Fourth *Quarter')) or s.find('b',text = re.compile('Third *Quarter'))
  or s.find('b',text = re.compile('Second *Quarter')) or s.find('b',text = re.compile('First *Quarter')) )
  if Qrow_condition:
    continue #Quarter passing
    print("Quarter detected!") #This won't be printed on

  #print(s) #Debugging code
  if not s.find('a',href = True):
    continue
  print("Text: ", "https://www.sec.gov" + s.find('a',href = True)['href'])
  print("ID:",s.find('td').text.strip())
  print("Date: ", wordTnum(s.find('td').findNextSibling().text).strip())
  f_Num1 = s.find('b',text = re.compile('File No'))
  f_Num2 = s.find('i',text = re.compile('File No'))
  f_Num3 = s.find('strong',text = re.compile('File No'))

  if f_Num1 and isinstance(f_Num1.nextSibling,NavigableString):
    print("File No: ", f_Num1.nextSibling.strip())
  elif f_Num2 and isinstance(f_Num2.nextSibling,NavigableString):
    print("File No: ", f_Num2.nextSibling.strip())
  elif f_Num3 and isinstance(f_Num3.nextSibling,NavigableString):
    print("File No: ", f_Num3.nextSibling.strip())
  else:
    counter += 1
    print("No File number")
    print("###############################################")
    continue
  print("==========================================")


  rule_id = s.find('td').text.strip()
  date = wordTnum(s.find('td').findNextSibling().text.strip())
  url = "https://www.sec.gov" + s.find('a',href = True)['href']
  if f_Num1 and isinstance(f_Num1.nextSibling,NavigableString):
    file_no = f_Num1.nextSibling.strip()
  elif f_Num2 and isinstance(f_Num2.nextSibling,NavigableString):
    file_no = f_Num2.nextSibling.strip()
  elif f_Num3 and isinstance(f_Num3.nextSibling,NavigableString):
    file_no = f_Num3.nextSibling.strip()

  if ',' in file_no:
    
    for each_fnum in file_no.split(", "):
      file = open('list.csv','a')
      writer = csv.writer(file)
      headers = ([each_fnum,rule_id, date, url,'','',''])
      writer.writerow(headers)
      file.close

  else:
    file = open('list.csv','a')
    writer = csv.writer(file)
    headers = ([file_no,rule_id, date, url,'','',''])
    writer.writerow(headers)
    file.close()

print("Number of Proposed rule without File No: ",counter)

"""# Matcher

Since the Rulemaking Index page lists each role by the file number, we don't need to run it like before 2013 one.

Simply make csv file from scrapper and run this matcher. It will find rules with same file number and fill in the column.

After run this matcher, you are all set! :)
"""

# url = 'https://raw.githubusercontent.com/ownssi/SEC-Rule/main/url.csv' #Just in case for Github
df = pd.read_csv('list.csv')
rule_id = df["Proposed Rule ID"].tolist()

#df = pd.read_csv('test.csv') #Not using colab
rule_id = df["Proposed Rule ID"].tolist()
df.fillna('', inplace=True)

url_to_scrape = "https://www.sec.gov/rules/rulemaking-index.shtml"

html_document = getdoc(url_to_scrape)
soup = BeautifulSoup(html_document)


for id in rule_id:

    target_proposed_rule = soup.find('span', text = id) #proposed rule ID on text argument
    print(id)
    mask = df["Proposed Rule ID"] == id

    try:
        list_of_rules = target_proposed_rule.find_parent().find_parent()
    except:
        continue

    i = 0
    child = list_of_rules.findChild()
    num_iterated = len(list_of_rules.findChild().find_next_siblings())+1
    while i < num_iterated:
        if child['class'][0] == 'final-rule':
            ID = child.find("span", {'class':'release'}).text
            DATE = child.find("span", {'class':'date'}).text[-2:]

            print("Rule URL: ", "https://www.sec.gov/rules/final/"+"20" + DATE + "/"+ ID +".pdf")
            rule_text = "https://www.sec.gov/rules/final/"+"20" + DATE + "/"+ ID +".pdf"
            df.loc[mask, "Final Rule Text"] = df["Final Rule Text"].astype(str) + rule_text + ', '

            print("Date is: ", DATE)
            date  = child.find("span", {'class':'date'}).text
            df.loc[mask, "Final Rule Date"] = df["Final Rule Date"].astype(str) + date + ', '
            
            print("ID is: ", child.find("span", {'class':'release'}).text)
            rule_id = child.find("span", {'class':'release'}).text
            df.loc[mask, "Final Rule ID"] = df["Final Rule ID"].astype(str) + rule_id + ', '
            
        i += 1
        child = child.find_next_sibling()

    print("====================================")

df.to_csv("list.csv", index=False)

"""# Data frame rigging"""

# load data
df = pd.read_csv('list.csv')

#Run if 1996 rule is contained
#df["Final Rule ID"][24] = "33-7431, "

#Run if 2018 is contained
#df["Final Rule ID"][53] = "34-86032;IA-5247, "
#df["Final Rule Text"][53] = "https://www.sec.gov/rules/final/2019/34-86032.pdf"

df['Final Rule ID']=df['Final Rule ID'].str[:-2]
df['Final Rule Date']=df['Final Rule Date'].str[:-2]
df['Final Rule Text']=df['Final Rule Text'].str[:-2]

df['Final Rule ID'] = df['Final Rule ID'].str.split(', ')
df['Final Rule Date'] = df['Final Rule Date'].str.split(', ')
df['Final Rule Text'] = df['Final Rule Text'].str.split(', ')

columns = ["Final Rule ID", "Final Rule Date", "Final Rule Text"]
df = df.explode(column=columns).reset_index(drop=False)

df.to_csv('formatted.csv', index = False)
display(df)